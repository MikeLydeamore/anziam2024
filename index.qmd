---
pagetitle: "conmat: Programmatic generation of synthetic contact matrices"
subtitle: "Going beyond Prem"
author: "Michael Lydeamore, Nick Tierney, Nick Golding"
email: "michael.lydeamore@monash.edu"
department: "Department of Econometrics and Business Statistics"
unit-url: "https://slides.michaellydeamore.com/anziam2024"
footer: "https://slides.michaellydeamore.com/anziam2024"
format: 
  revealjs:
    logo: images/monash-stacked-blue-rgb-transparent.png
    slide-number: true
    multiplex: false
    theme: assets/monash.scss
    show-slide-number: all
    show-notes: false
    controls: true
    width: 1280
    height: 750
    css: [assets/tachyons-addon.css, assets/custom.css, assets/lecture-01.css]
    include-after-body: "assets/after-body.html"
    chalkboard:
      boardmarker-width: 5
      buttons: true
---


```{r, include = FALSE}
current_file <- knitr::current_input()
basename <- gsub(".[Rq]md$", "", current_file)

knitr::opts_chunk$set(
  fig.path = sprintf("images/%s/", basename),
  fig.width = 6,
  fig.height = 4,
  fig.align = "center",
  out.width = "100%",
  fig.retina = 3,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.path = "cache/"
)

library(dplyr)
library(ggplot2)
library(tidyr)
library(deSolve)
library(conmat)
```

## <br>[`conmat`: Programmatic generation of synthetic contact matrices]{.monash-blue .title} {#etc5513-title background-image="images/bg-01.png"}

### `r rmarkdown::metadata$subtitle`

`r rmarkdown::metadata$author`

`r rmarkdown::metadata$department`

::: tl
<br>

<ul class="fa-ul">

<li>

[<i class="fas fa-envelope"></i>]{.fa-li}`r rmarkdown::metadata$email`

</li>

<li>

[<i class="fa-solid fa-globe"></i>]{.fa-li}<a href="`r rmarkdown::metadata[["unit-url"]]`">`r rmarkdown::metadata[["unit-url"]]`</a>

</li>

</ul>

<br>
:::

## Contact matrices

* Some infectious diseases are spread from contact between individuals
* When we model disease, we need some idea of how often people contact other people
* Typically, we turn to _contact diary studies_

## Contact Diary Studies

Contact diary studies follow individuals and ask them:

* how many contacts of a certain type, 
* the duration of contact,
* the location of the contact,
* the frequency of the contact

::: {.fragment}
They are undoubtedly the gold standard for data on the number of contacts.

But they are prohibitively expensive, and logistically very challenging.
:::

## POLYMOD

Published in 2008, Mossong et. al undertook a contact diary study of 7290 participants across Europe.

_It remains the most widely cited contact diary study_.

![](images/mossong-combined.png)

## Contact matrix projection

Prem et. al formed a model that predicts the number of contacts based on the age of participants.

This model then allows for prediction to settings that are outside the original POLYMOD countries.

## What is provided:

::: {.columns}
::: {.column width='50%'}
![](images/prem-dir-mats.png){fig-align="center"}

:::
::: {.column width='50%'}
![](images/prem-spreadsheet.png){fig-align="center"}

:::
:::

## What is provided:

Estimates for 155 countries.

There is code, but it is not easily expandable to new settings.

::: {.fragment}
If your country isn't in the list of 155: no estimates for you.
:::

::: {.fragment}
::: {.callout-caution}
Australia was in the original set of 155, but was **not** included in the updated estimates a few years ago.
:::
:::

::: {.fragment}
There are other more technical concerns with components of the model that probably have minor influence on the projects.
:::




## An alternative approach

We developed an *open-source* approach, that can be applied to any input population (big or small).

You can view the package online: [https://www.github.com/idem-lab/conmat](https://www.github.com/idem-lab/conmat)

or install using:

```{r}
#| eval: false

remotes::install_github("idem-lab/conmat")
```

## Model details

Fundamentally, these models have pretty little to go on. They are trained on the number of contacts between two age groups $i$ and $j$. In `conmat`, this takes the form:

$$\begin{aligned}
c_{ij} = \beta_0 + \beta_1(|i-j|) + &\beta_2(|i-j|^2) + \beta_3(i\times j) + \beta_4(i+j) +\\ &\beta_5\max(i, j) + \beta_6\min(i, j)
\end{aligned}
$$

::: {.fragment .callout-tip}
We want to fit this as a generalised additive model, so _actually_ we have splines on these terms to satisfy the smoothness requirements.

This is very similar to the approach by Prem et. al who perform post-hoc smoothing of the parameters after MCMC.
:::

## Model details

* All that is needed to make predictions is the number of individuals in different age groups. 
* This is easily obtainable information from most statistics bureaus/governments.

## Data sources {.smaller}

::: {.columns}
::: {.column width='50%'}
* The `socialmixr` package has this information for most countries:

```{r}
socialmixr::wpp_age("Australia", 2015)
```
:::
::: {.column width='50%' .fragment}
And for inside Australia, `conmat` provides some ABS functions:

```{r}
conmat::abs_age_lga("Melbourne (C)")
```
:::
:::

## Testing model fit: SIR Model

To test model fit, we will use a _structured, SIR model_. For the population in group $i$,

$$ \lambda(t) = \beta \times C \times I(t),$$
where 

* $\beta$ represents the probability of transmission given contact (here assumed to be independent of age group)
* $C$ is an $N \times N$ matrix with element $(i,j)$ representing the number of contacts _from_ age group $i$ _to_ age group $j$, and
* $I(t)$ is a $N \times 1$ vector of the number of infected individuals

## Testing model fit: SIR Model

Then,

$$\begin{aligned}
\frac{dS}{dt} &= -\lambda(t) S I \\
\frac{dI}{dt} &= \lambda(t) S I - \gamma I \\
\frac{dR}{dt} &= \gamma I
\end{aligned}
$$

and the total population size is conserved ($S+I+R = N$).

```{r}
#| echo: false
tidy_ode <- function(ode_soln) {
    ode_soln %>%
        pivot_longer(cols = -time) %>%
        mutate(parent_state = substr(name, 1, 1)) %>%
        group_by(time, parent_state) %>%
        summarise(value = sum(value)) %>%
        ungroup() %>%
        rename(name = parent_state)
}

age_breaks <- c(seq(0, 75, by = 5), Inf)

n_finite_states <- length(age_breaks) - 1
polymod_self_fit <- extrapolate_polymod(
    population = get_polymod_population(),
    age_breaks = age_breaks
)

transform_polymod_data <- function(setting) {
    get_polymod_contact_data(setting = setting) %>%
        mutate(contacts = contacts / participants) %>%
        aggregate_predicted_contacts(population = get_polymod_population()) %>%
        tidyr::pivot_wider(
            names_from = age_group_from,
            values_from = contacts
        ) %>%
        tibble::column_to_rownames(
            "age_group_to"
        ) %>%
        as.matrix()
}

polymod_data <- list(
    home = transform_polymod_data("home"),
    work = transform_polymod_data("work"),
    school = transform_polymod_data("school"),
    other = transform_polymod_data("other"),
    all = transform_polymod_data("all")
)

polymod_data_matrix <- transmission_probability_matrix(
    home = polymod_data$home,
    work = polymod_data$work,
    school = polymod_data$school,
    other = polymod_data$other,
    age_breaks = age_breaks
)


S0 <- get_polymod_population()$population[1:n_finite_states]
I0 <- rep(1, times = n_finite_states)
R0 <- rep(0, times = n_finite_states)
initial_condition <- c(S0, I0, R0)
names(initial_condition) <- paste(
    rep(c("S0", "I0", "R0"), each = n_finite_states),
    age_breaks[1:n_finite_states],
    sep = "_"
)


age_structured_sir <- function(time, state, parameters) {
    # Calculate the force of infection for each setting:
    # unstructured SIR beta is age_group_n / pop_n

    N_by_age <- map_dbl(
        .x = parameters$s_indexes,
        .f = function(i) {
            current_indexes_to_sum <- c(
                parameters$s_indexes[i],
                parameters$i_indexes[i],
                parameters$r_indexes[i]
            )
            sum(state[current_indexes_to_sum])
        }
    )

    # normalise by the age population
    N_infected_by_age <- state[parameters$i_indexes] / N_by_age

    # functional method for takign the product of two matrices
    product <- function(transmission, contact) {
        map2(transmission, contact, `*`)
    }

    age_normalise <- function(beta) {
        # matrix multiply by infected and normalise by age population
        map(beta, function(beta) {
            beta %*% N_infected_by_age
        })
    }

    lambdas <- tibble(
        setting = names(parameters$transmission_matrix),
        transmission_matrix = parameters$transmission_matrix,
        homogeneous_contact = parameters$homogeneous_contact[1:4]
    ) %>%
        mutate(
            beta = product(transmission_matrix, homogeneous_contact),
            lambda = age_normalise(beta)
        )

    # Combine them all into one term for ease of computation
    lambda_total <- Reduce("+", lambdas$lambda)

    # Don't forget to normalise your infection rate by the population!
    dSdt <- -lambda_total * state[parameters$s_indexes]

    dIdt <- lambda_total * state[parameters$s_indexes] -
        parameters$gamma * state[parameters$i_indexes]

    dRdt <- parameters$gamma * state[parameters$i_indexes]

    return(
        list(
            c(
                dSdt,
                dIdt,
                dRdt
            )
        )
    )
}

calculate_R0 <- function(multiplier, R0_target, transmission_matrices, contact_matrices) {
    total_matrix <- transmission_matrices$home * contact_matrices$home + transmission_matrices$work * contact_matrices$work + transmission_matrices$school * contact_matrices$school + transmission_matrices$other * contact_matrices$other

    abs(Re(eigen(total_matrix * multiplier)$values[1]) - R0_target)
}

```


## Difference in peak size compared to POLYMOD

* Depends on R0 but contact model estimates are _systematically lower_
* Possibly because of unequal sampling which isn't accounted for in data

```{r sweep-R0}
#| echo: false
#| cache: true

polymod_transimssion_rate <- 0.08
polymod_transmission_matrix <- transmission_probability_matrix(
    home = matrix(polymod_transimssion_rate, nrow = n_finite_states, ncol = n_finite_states),
    work = matrix(polymod_transimssion_rate, nrow = n_finite_states, ncol = n_finite_states),
    school = matrix(polymod_transimssion_rate, nrow = n_finite_states, ncol = n_finite_states),
    other = matrix(polymod_transimssion_rate, nrow = n_finite_states, ncol = n_finite_states),
    age_breaks = age_breaks
)
R0_range <- seq(1.1, 2.5, by = 0.05)

times <- seq(0, 200, by = 0.05)

R0_sweep_finalsize <- lapply(R0_range, function(R0) {
    # Model fit
    self_fit_R0_multiplier <- optimize(f = calculate_R0, interval = c(0.01, 5), transmission_matrices = polymod_transmission_matrix, contact_matrices = polymod_self_fit, R0_target = R0)$minimum

    parameters <- list(
        "transmission_matrix" = polymod_transmission_matrix,
        "homogeneous_contact" = lapply(polymod_self_fit, `*`, self_fit_R0_multiplier),
        "gamma" = 1,
        "s_indexes" = 1:n_finite_states,
        "i_indexes" = (n_finite_states + 1):(2 * n_finite_states),
        "r_indexes" = (2 * n_finite_states + 1):(3 * n_finite_states)
    )

    polymod_model_fit_soln <- ode(
        y = initial_condition,
        times = times,
        func = age_structured_sir,
        parms = parameters
    ) %>%
        as.data.frame() %>%
        as_tibble()

    final_size_model_fit <- tidy_ode(polymod_model_fit_soln) %>%
        filter(name == "I") %>%
        filter(value == max(value)) %>%
        pull(value)

    ## Raw data
    data_R0_multiplier <- optimize(f = calculate_R0, interval = c(0.01, 5), transmission_matrices = polymod_transmission_matrix, contact_matrices = polymod_data_matrix, R0_target = R0)$minimum

    parameters_data <- parameters
    parameters_data$homogeneous_contact <- lapply(polymod_data_matrix, `*`, data_R0_multiplier)
    polymod_data_soln <- ode(
        y = initial_condition,
        times = times,
        func = age_structured_sir,
        parms = parameters_data
    ) %>%
        as.data.frame() %>%
        as_tibble()

    final_size_data <- tidy_ode(polymod_data_soln) %>%
        filter(name == "I") %>%
        filter(value == max(value)) %>%
        pull(value)

    prem_R0_multiplier <- optimize(f = calculate_R0, interval = c(0.01, 5), transmission_matrices = polymod_transmission_matrix, contact_matrices = prem_germany_contact_matrices, R0_target = R0)$minimum

    parameters_data$homogeneous_contact <- lapply(prem_germany_contact_matrices, `*`, prem_R0_multiplier)
    prem_soln <- ode(
        y = initial_condition,
        times = times,
        func = age_structured_sir,
        parms = parameters_data
    ) %>%
        as.data.frame() %>%
        as_tibble()

    peak_size_prem <- tidy_ode(prem_soln) %>%
        filter(name == "I") %>%
        filter(value == max(value)) %>%
        pull(value)

    tibble(final_size_data = final_size_data, final_size_model_fit = final_size_model_fit, peak_size_prem = peak_size_prem, R0 = R0)
}) %>% bind_rows()

```

::: {.r-stack}
```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 5

R0_sweep_finalsize %>%
    mutate(prop_data = final_size_data / sum(initial_condition), prop_fit = final_size_model_fit / sum(initial_condition)) %>%
    mutate(percentage_difference = (prop_data - prop_fit) / prop_data * 100) %>%
    ggplot(aes(x = R0, y = percentage_difference)) +
    geom_point() +
    theme_bw() +
    labs(x = "R0", y = "Relative difference in\npeak size") +
    ylim(0, NA) +
    geom_hline(yintercept = 0, linetype = "dashed")
```

![](https://media.giphy.com/media/xTiQyBOIQe5cgiyUPS/giphy.gif){fig-align="center" .fragment}

:::

## Difference in peak size compared to Prem {.smaller}

* `conmat` method systematically gives _larger_ epidemics compared to Prem et. al
* Could be due to different interpretations of parameters or slightly different data sources

::: {.r-stack}
```{r}
#| echo: false
#| fig-height: 2
#| fig-width: 5

R0_sweep_finalsize %>%
    mutate(prop_prem = peak_size_prem / sum(initial_condition), prop_fit = final_size_model_fit / sum(initial_condition)) %>%
    mutate(percentage_difference = (prop_prem - prop_fit) / prop_fit * 100) %>%
    ggplot(aes(x = R0, y = percentage_difference)) +
    geom_point() +
    theme_bw() +
    labs(x = "R0", y = "Percentage difference \nbetween Prem and conmat") +
    geom_hline(yintercept = 0, linetype = "dashed")
```

![](https://media.giphy.com/media/xTiQyBOIQe5cgiyUPS/giphy.gif){fig-align="center" .fragment}

:::

## For a fixed R0=1.5

```{r}
#| echo: false

library(colorspace)
R0 <- 1.5

self_fit_R0_multiplier <- optimize(f = calculate_R0, interval = c(0.01, 5), transmission_matrices = polymod_transmission_matrix, contact_matrices = polymod_self_fit, R0_target = R0)$minimum

self_fit_contact_matrices <- lapply(polymod_self_fit, `*`, self_fit_R0_multiplier)

prem_R0_multiplier <- optimize(f = calculate_R0, interval = c(0.01, 5), transmission_matrices = polymod_transmission_matrix, contact_matrices = prem_germany_contact_matrices, R0_target = R0)$minimum

prem_contact_matrices <- lapply(prem_germany_contact_matrices, function(m) {
    m <- m * prem_R0_multiplier

    rownames(m) <- rownames(self_fit_contact_matrices$home)
    colnames(m) <- rownames(self_fit_contact_matrices$home)

    return(m)
})

plot_matrix_diff <- function(matrix_1, matrix_2) {
    
    combined_diffs <- lapply(seq_along(matrix_1), function(i) {
        diff_matrix <- matrix_1[[i]] - matrix_2[[i]]

        diff_matrix %>%
            matrix_to_predictions() %>%
            mutate(type = names(matrix_1)[i])
    }) %>% bind_rows()

    combined_diffs %>%
    filter(type != "all") %>%
    ggplot(aes(x=age_group_from, y=age_group_to, fill = contacts)) +
    geom_tile() +
    coord_fixed() +
    scale_fill_continuous_diverging(palette = "Blue-Red 3") +
    facet_wrap(~type) +
    theme_minimal() +
    labs(x="Age Group (from)", y="Age Group (to)", fill = "# of contacts") +
    theme(axis.text.x = element_blank(), axis.text.y=element_blank())
    
}

plot_matrix_diff(prem_contact_matrices, self_fit_contact_matrices)


```

## Why not just use Prem?

* There's no ground truth in these projects, making comparisons to almost impossible

::: {.fragment}
![](images/chatgpt.jpg){fig-align="center"}
:::



## Why should I use this? It's easy!

Getting a contact matrix is easy:

```{r}
world_data <- socialmixr::wpp_age()
population <- age_population(
    data = world_data,
    location_col = country,
    location = "Australia",
    age_col = lower.age.limit,
    year_col = year,
    year = 2015
)

population
```

## Why should I use this?

Getting a contact matrix is easy:


```{r}
#| cache: true
contact_rates <- extrapolate_polymod(
    population = population,
    age_breaks = c(seq(0, 100, by = 5), Inf)
)

print(contact_rates)
```

## Why should I use this?

```{r}
contact_rates$home
```

## Why should I use this? You can use a custom population


```{r}
#| code-line-numbers: "1-6|8-15|17-20"
flat_population_data <- tibble(
    country = "flatland",
    year = 2024,
    population = 1000,
    lower.age.limit = seq(0, 75, by=5)
)

flat_population <- age_population(
    data = flat_population_data,
    location_col = country,
    location = "flatland",
    age_col = lower.age.limit,
    year_col = year,
    year = 2024
)

flat_contact_rates <- extrapolate_polymod(
    population = flat_population,
    age_breaks = c(seq(0, 75, by=5), Inf)
)
```

## Why should I use this? You can use a custom population

```{r}
flat_contact_rates$home
```

## Conclusion

::: {.incremental}

* Contact matrices are frequently used in infectious diseases modelling
* Standard Prem et. al matrices are not written for re-use
* `conmat` provides an interface to easily generate and update models from any data source
* Can integrate vaccination, calculate next generation matrices, and a handful of other features

:::

::: {.fragment}
And it's available **right now**

[https://www.github.com/idem-lab/conmat](https://www.github.com/idem-lab/conmat)
:::

## Acknowledgements

::: {.columns}
::: {.column width='50%'}
#### Telethon Kids Institute

* Nick Tierney
* Nick Golding
* Aarathy Babu
:::
::: {.column width='50%'}

* SPECTRUM-SPARK Travel Funding
* Department of Econometrics and Business Statistics, Monash University
:::
:::

